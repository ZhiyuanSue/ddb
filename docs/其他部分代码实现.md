# user 和article表是可以直接载入的
在读入sql文件的时候需要考虑分片，因此，只需要读入的时候加以判断,以User表作为例子
```
		if (line.trim().endsWith(";")) {  
			boolean can_append=true;
			if(line.startsWith("  (\"")){
				String str = line.toString();
				String[] parts = str.split(",");
				if(parts[10].equals(" \"Hong Kong\"") && dbms_num!=3){
					can_append=false;
				}
				if(parts[10].equals(" \"Beijing\"") && dbms_num!=2){
					can_append=false;
				}
			}
			if (can_append){
				sqlScript.append(line).append("\n");  
			}else{
				sqlScript.append("(\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\");\n");  
			} 
			...
		}else {  
			boolean can_append=true;
			if(line.startsWith("  (\"")){
				String str = line.toString();
				String[] parts = str.split(",");
				if(parts[10].equals(" \"Hong Kong\"") && dbms_num!=3){
					can_append=false;
				}
				if(parts[10].equals(" \"Beijing\"") && dbms_num!=2){
					can_append=false;
				}
			}
			if (can_append){
				sqlScript.append(line).append("\n");  
			}
		}  
```
对于最后一行，因为无法删除，所以插入了一个空的词条
article表也是类似的逻辑

# user_read表
user read表就和上面两个表开始不一样了，他在读入的时候，需要读出user table的字段并因此判断如何分片。

一个简化的设计在于，因为他和user表的分片是一样的，只需要查找，在对应数据库的user表中的uid这一列是否存在该数字即可。
如果存在，就插入到对应的dbms。

所以一开始就对uid进行了查询，并且将结果插入到了一个hashset中，从而快速查找对应的dbms是否存在相应的uid

# be_read表
同样的，他在bulk载入的时候，同样需要考虑article的分片情况，读出article的字段，从此判断如何分片

除此之外，由于没有现成的资源，需要我们手动生成相应的be_read表。
readNum表示某一本书被阅读的次数，这是需要查询的
除此之外，需要考虑一个事情，因为read表已经被我们分片了，所以需要分别查询两个表。
需要查询的内容
需要从article查询aid，以及category。结果存入hash map中。由于dbms1（hadoop2节点）包含了category的两种情况的所有数据，所以只需要查询一次即可。
需要从read表读入uid, aid, aggreeOrNot, commentOrNot,shareOrNot，
然后根据查询的结果，查找hashmap的结果，生成readUidList，commentNum，commentUidList，agreeNum, agreeUidList, shareNum, shareUidList这若干个字段，并存放到不同的数据库中。