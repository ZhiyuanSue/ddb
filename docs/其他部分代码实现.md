# user 和article表是可以直接载入的
在读入sql文件的时候需要考虑分片，因此，只需要读入的时候加以判断,以User表作为例子
```
		if (line.trim().endsWith(";")) {  
			boolean can_append=true;
			if(line.startsWith("  (\"")){
				String str = line.toString();
				String[] parts = str.split(",");
				if(parts[10].equals(" \"Hong Kong\"") && dbms_num!=3){
					can_append=false;
				}
				if(parts[10].equals(" \"Beijing\"") && dbms_num!=2){
					can_append=false;
				}
			}
			if (can_append){
				sqlScript.append(line).append("\n");  
			}else{
				sqlScript.append("(\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\");\n");  
			} 
			...
		}else {  
			boolean can_append=true;
			if(line.startsWith("  (\"")){
				String str = line.toString();
				String[] parts = str.split(",");
				if(parts[10].equals(" \"Hong Kong\"") && dbms_num!=3){
					can_append=false;
				}
				if(parts[10].equals(" \"Beijing\"") && dbms_num!=2){
					can_append=false;
				}
			}
			if (can_append){
				sqlScript.append(line).append("\n");  
			}
		}  
```
对于最后一行，因为无法删除，所以插入了一个空的词条
article表也是类似的逻辑

# user_read表
user read表就和上面两个表开始不一样了，他在读入的时候，需要读出user table的字段并因此判断如何分片。

一个简化的设计在于，因为他和user表的分片是一样的，只需要查找，在对应数据库的user表中的uid这一列是否存在该数字即可。
如果存在，就插入到对应的dbms。

所以一开始就对uid进行了查询，并且将结果插入到了一个hashset中，从而快速查找对应的dbms是否存在相应的uid

# be_read表
同样的，他在bulk载入的时候，同样需要考虑article的分片情况，读出article的字段，从此判断如何分片

除此之外，由于没有现成的资源，需要我们手动生成相应的be_read表。
readNum表示某一本书被阅读的次数，这是需要查询的
除此之外，需要考虑一个事情，因为read表已经被我们分片了，所以需要分别查询两个表。
需要查询的内容
需要从article查询aid，以及category。结果存入hash map中。由于dbms1（hadoop2节点）包含了category的两种情况的所有数据，所以只需要查询一次即可。
需要从read表读入uid, aid, aggreeOrNot, commentOrNot,shareOrNot，
然后根据查询的结果，查找hashmap的结果，生成readUidList，commentNum，commentUidList，agreeNum, agreeUidList, shareNum, shareUidList这若干个字段，并存放到不同的数据库中。

# popular_rank表
这个表要问我dayly，weekly，monthly的数据，所以一个问题在于，我需要确定他到底，每个时间属于哪几个周。

好在，在be_read表中，我打印了一圈数据，他的timestamp应该都是已经排好序了，而且相邻两个，相差10s。
既然如此，一个简单的办法是，保留一个curr_day,curr_week,curr_month时间戳，向下遍历，如果不再属于当前day/week/，就更新，然后维护一个表格，排序即可。从而获取每天，每周，每月的top5的article的aid,
然后需要关注他的字段，也就是timestamp, temporalGranularity, articleAidList。
我对此的理解是，使用timestemp记录每一天/周/月的开始时间，后面的temporalGranularity表示时间粒度，而articleAidList则是记录从该时间开始的top5的article。

还需要考虑的是，如何定义popular。这件事情，我们做的简单一些，只考虑被阅读的次数

最后，需要将hashmap中的数据，再分别按照分片，分别存入两个数据库中。

# 从hadoop获取具体的article的content并且使用redis作为缓存。